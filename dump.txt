// docker-compose.yml
version: '3.8'
services:
  elasticsearch:
    image: 'elasticsearch:7.16.1'
    container_name: elasticsearch
    environment:
      discovery.type: single-node
      ES_JAVA_OPTS: '-Xms512m -Xmx512m'
      xpack.security.enabled: 'true'
      ELASTIC_PASSWORD: '${ELASTIC_PASSWORD}'
      KIBANA_SYSTEM_PASSWORD: ${KIBANA_SYSTEM_PASSWORD:-}
      METRICBEAT_INTERNAL_PASSWORD: ${METRICBEAT_INTERNAL_PASSWORD:-}
      FILEBEAT_INTERNAL_PASSWORD: ${FILEBEAT_INTERNAL_PASSWORD:-}
      HEARTBEAT_INTERNAL_PASSWORD: ${HEARTBEAT_INTERNAL_PASSWORD:-}
      MONITORING_INTERNAL_PASSWORD: ${MONITORING_INTERNAL_PASSWORD:-}
      BEATS_SYSTEM_PASSWORD: ${BEATS_SYSTEM_PASSWORD:-}
      LOGSTASH_INTERNAL_PASSWORD: ${LOGSTASH_INTERNAL_PASSWORD:-}
      
    ports:
      - '9200:9200'
      - '9300:9300'
    healthcheck:
      test:
        - CMD-SHELL
        - 'curl --silent --fail -u elastic:changeme localhost:9200/_cluster/health || exit 1'
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - elk
    restart: unless-stopped
    logging:
      driver: fluentd
      options:
        fluentd-address: 'tcp://127.0.0.1:24224'
        fluentd-async: 'true'
        fluentd-sub-second-precision: 'true'

  logstash:
    image: 'logstash:7.16.1'
    container_name: logstash
    environment:
      discovery.seed_hosts: logstash
      LC_ALL: "en_US.UTF-8"
      LS_JAVA_OPTS: '-Xms512m -Xmx512m'
      LOGSTASH_INTERNAL_PASSWORD: ${LOGSTASH_INTERNAL_PASSWORD:-}
      KIBANA_SYSTEM_PASSWORD: ${KIBANA_SYSTEM_PASSWORD:-}
      ELASTIC_PASSWORD: '${ELASTIC_PASSWORD}'
      METRICBEAT_INTERNAL_PASSWORD: ${METRICBEAT_INTERNAL_PASSWORD:-}
      FILEBEAT_INTERNAL_PASSWORD: ${FILEBEAT_INTERNAL_PASSWORD:-}
      HEARTBEAT_INTERNAL_PASSWORD: ${HEARTBEAT_INTERNAL_PASSWORD:-}
      MONITORING_INTERNAL_PASSWORD: ${MONITORING_INTERNAL_PASSWORD:-}
      BEATS_SYSTEM_PASSWORD: ${BEATS_SYSTEM_PASSWORD:-}
      
    volumes:
      - './logstash/logstash-nginx.config:/usr/share/logstash/pipeline/logstash-nginx.config'
    ports:
      - '5000:5000/tcp'
      - '5000:5000/udp'
      - '5044:5044'
      - '9600:9600'
    depends_on:
      - elasticsearch
    networks:
      - elk
    restart: unless-stopped
    logging:
      driver: fluentd
      options:
        fluentd-address: 'tcp://127.0.0.1:24224'
        fluentd-async: 'true'
        fluentd-sub-second-precision: 'true'
    command: 'logstash -f /usr/share/logstash/pipeline/logstash-nginx.config'

  kibana:
    image: 'kibana:7.16.1'
    container_name: kibana
    environment:
      ELASTICSEARCH_HOSTS: 'http://elasticsearch:9200'
      LOGSTASH_INTERNAL_PASSWORD: ${LOGSTASH_INTERNAL_PASSWORD:-}
      KIBANA_SYSTEM_PASSWORD: ${KIBANA_SYSTEM_PASSWORD:-}
      ELASTIC_PASSWORD: '${ELASTIC_PASSWORD}'
      METRICBEAT_INTERNAL_PASSWORD: ${METRICBEAT_INTERNAL_PASSWORD:-}
      FILEBEAT_INTERNAL_PASSWORD: ${FILEBEAT_INTERNAL_PASSWORD:-}
      HEARTBEAT_INTERNAL_PASSWORD: ${HEARTBEAT_INTERNAL_PASSWORD:-}
      MONITORING_INTERNAL_PASSWORD: ${MONITORING_INTERNAL_PASSWORD:-}
      BEATS_SYSTEM_PASSWORD: ${BEATS_SYSTEM_PASSWORD:-}
      
      XPACK_SECURITY_ENCRYPTIONKEY: ${KIBANA_SECURITY_KEY}
      XPACK_REPORTING_ENCRYPTIONKEY: ${KIBANA_REPORTING_KEY}
      XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY: ${KIBANA_ENCRYPTEDSAVEDOBJECTS_KEY}

    ports:
      - '5601:5601'
    volumes:
      - ./kibana/kibana.yml:/usr/share/kibana/config/kibana.yml:ro,Z
    depends_on:
      - elasticsearch
    networks:
      - elk
    restart: unless-stopped
    logging:
      driver: fluentd
      options:
        fluentd-address: 'tcp://127.0.0.1:24224'
        fluentd-async: 'true'
        fluentd-sub-second-precision: 'true'

networks:
  elk:
    driver: bridge


// kibana/kibana.yml
---
server.name: kibana
server.host: 0.0.0.0
elasticsearch.hosts: [ https://elastic.monitoring.6d.com.sa ]

monitoring.ui.container.elasticsearch.enabled: true
monitoring.ui.container.logstash.enabled: true

elasticsearch.username: kibana_system
elasticsearch.password: ${KIBANA_SYSTEM_PASSWORD}


// logstash/logstash-nginx.config
input {
  tcp {
    port => 5000
    type => syslog
  }
  udp {
    port => 5000
    type => syslog
  }
  beats {
    port => 5044
  }
}

filter {
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGLINE}" }
    }
    date {
      match => [ "timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
}

output {
  elasticsearch {
    hosts => ["https://elastic.monitoring.6d.com.sa"]
    index => "logstash-%{+YYYY.MM.dd}"
  }
  stdout { codec => rubydebug }
}